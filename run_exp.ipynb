{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"run_exp.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMq7yad02n4ZSMOAPJMCnz/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGwsehA3mGe_","executionInfo":{"status":"ok","timestamp":1636470966369,"user_tz":-480,"elapsed":27005,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}},"outputId":"b6d72819-0866-49cf-c245-3dedbe7da0a9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ON6ji5XymXZL","executionInfo":{"elapsed":830422,"status":"ok","timestamp":1636463140213,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"},"user_tz":-480},"outputId":"42917cad-a290-47f2-ec1d-9689086ab699"},"source":["%cd '/content/drive/MyDrive/EEG project/apex'\n","!pip install -v --no-cache-dir --global-option=\"--pyprof\" --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n","%cd .."],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/EEG project/apex\n","/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n","  cmdoptions.check_install_build_global(options)\n","Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n","Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/lib/python3.7/dist-packages\n","sysconfig: /usr/lib/python3.7/site-packages\n","Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/lib/python3.7/dist-packages\n","sysconfig: /usr/lib/python3.7/site-packages\n","Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/include/python3.7/UNKNOWN\n","sysconfig: /usr/include/python3.7m/UNKNOWN\n","Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/bin\n","sysconfig: /usr/bin\n","Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local\n","sysconfig: /usr\n","Additional context:\n","user = False\n","home = None\n","root = None\n","prefix = None\n","Non-user install because site-packages writeable\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-v0j8ygu_\n","Created temporary directory: /tmp/pip-req-tracker-rfq01vam\n","Initialized build tracking at /tmp/pip-req-tracker-rfq01vam\n","Created build tracker: /tmp/pip-req-tracker-rfq01vam\n","Entered build tracker: /tmp/pip-req-tracker-rfq01vam\n","Created temporary directory: /tmp/pip-install-7g9m12mu\n","Processing /content/drive/MyDrive/EEG project/apex\n","  Created temporary directory: /tmp/pip-req-build-yowt5fg5\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","  Added file:///content/drive/MyDrive/EEG%20project/apex to build tracker '/tmp/pip-req-tracker-rfq01vam'\n","    Running setup.py (path:/tmp/pip-req-build-yowt5fg5/setup.py) egg_info for package from file:///content/drive/MyDrive/EEG%20project/apex\n","    Created temporary directory: /tmp/pip-pip-egg-info-b9vijapk\n","    Running command python setup.py egg_info\n","\n","\n","    torch.__version__  = 1.9.0+cu111\n","\n","\n","    running egg_info\n","    creating /tmp/pip-pip-egg-info-b9vijapk/apex.egg-info\n","    writing /tmp/pip-pip-egg-info-b9vijapk/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-pip-egg-info-b9vijapk/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-pip-egg-info-b9vijapk/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-pip-egg-info-b9vijapk/apex.egg-info/SOURCES.txt'\n","    adding license file 'LICENSE'\n","    writing manifest file '/tmp/pip-pip-egg-info-b9vijapk/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-yowt5fg5/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-yowt5fg5 has version 0.1, which satisfies requirement apex==0.1 from file:///content/drive/MyDrive/EEG%20project/apex\n","  Removed apex==0.1 from file:///content/drive/MyDrive/EEG%20project/apex from build tracker '/tmp/pip-req-tracker-rfq01vam'\n","Created temporary directory: /tmp/pip-unpack-1jefk39o\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/lib/python3.7/dist-packages\n","  sysconfig: /usr/lib/python3.7/site-packages\n","  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/lib/python3.7/dist-packages\n","  sysconfig: /usr/lib/python3.7/site-packages\n","  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/include/python3.7/apex\n","  sysconfig: /usr/include/python3.7m/apex\n","  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local/bin\n","  sysconfig: /usr/bin\n","  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","  distutils: /usr/local\n","  sysconfig: /usr\n","  Additional context:\n","  user = False\n","  home = None\n","  root = None\n","  prefix = None\n","  Created temporary directory: /tmp/pip-record-5i3pgpnb\n","    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-yowt5fg5/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-yowt5fg5/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --pyprof --cpp_ext --cuda_ext install --record /tmp/pip-record-5i3pgpnb/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/apex\n","\n","\n","    torch.__version__  = 1.9.0+cu111\n","\n","\n","    /tmp/pip-req-build-yowt5fg5/setup.py:58: DeprecationWarning:\n","\n","    Pyprof has been moved to its own dedicated repository and will soon be removed from Apex.  Please visit\n","    https://github.com/NVIDIA/PyProf\n","    for the latest version.\n","      warnings.warn(string, DeprecationWarning)\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2020 NVIDIA Corporation\n","    Built on Mon_Oct_12_20:09:46_PDT_2020\n","    Cuda compilation tools, release 11.1, V11.1.105\n","    Build cuda_11.1.TC455_06.29190527_0\n","    from /usr/local/cuda/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.7\n","    creating build/lib.linux-x86_64-3.7/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.7/apex\n","    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.7/apex\n","    creating build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.7/apex/transformer\n","    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer\n","    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.7/apex/transformer\n","    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.7/apex/transformer\n","    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer\n","    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.7/apex/transformer\n","    creating build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n","    creating build/lib.linux-x86_64-3.7/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.7/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof\n","    creating build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.7/apex/amp\n","    creating build/lib.linux-x86_64-3.7/apex/fused_dense\n","    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.7/apex/fused_dense\n","    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.7/apex/fused_dense\n","    creating build/lib.linux-x86_64-3.7/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.7/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.7/apex/normalization\n","    creating build/lib.linux-x86_64-3.7/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.7/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.7/apex/mlp\n","    creating build/lib.linux-x86_64-3.7/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n","    creating build/lib.linux-x86_64-3.7/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib\n","    creating build/lib.linux-x86_64-3.7/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.7/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.7/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.7/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.7/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.7/apex/RNN\n","    creating build/lib.linux-x86_64-3.7/apex/transformer/testing\n","    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n","    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n","    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n","    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n","    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.7/apex/transformer/testing\n","    creating build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n","    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n","    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n","    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n","    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel\n","    creating build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel\n","    creating build/lib.linux-x86_64-3.7/apex/transformer/functional\n","    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/functional\n","    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.7/apex/transformer/functional\n","    creating build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n","    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n","    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n","    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n","    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n","    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules\n","    creating build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n","    creating build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.7/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/transducer\n","    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n","    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/fmha\n","    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n","    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n","    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n","    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n","    copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n","    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n","    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n","    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n","    creating build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n","    running build_ext\n","    /usr/local/lib/python3.7/dist-packages/torch/utils/cpp_extension.py:370: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.7\n","    creating build/temp.linux-x86_64-3.7/csrc\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/include/python3.7m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/amp_C_frontend.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.7/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/syncbn.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/welford.cu -o build/temp.linux-x86_64-3.7/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/syncbn.o build/temp.linux-x86_64-3.7/csrc/welford.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:152:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:174:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:216:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:217:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:242:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:243:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:7,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/macros/Macros.h:195:64: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                    ^~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/c10/util/Exception.h:430:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.7/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    csrc/mlp.cpp: In function â€˜std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)â€™:\n","    csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of â€˜reserved_sizeâ€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of â€˜reserved_sizeâ€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","    csrc/mlp.cpp:67:59: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n","                                                               ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:69:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:221:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                               \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:223:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n","                                                            ^\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:121:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:78:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:78:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:78:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:69:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function â€˜std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)â€™:\n","    csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:121:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:124:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:221:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                               \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:223:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n","                                                            ^\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:121:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:138:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:138:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:138:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:140:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:138:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:138:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:138:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:140:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:138:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:138:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:138:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:140:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:124:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/mlp.o build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so\n","    building 'fused_dense_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.7/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    csrc/fused_dense.cpp: In function â€˜at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)â€™:\n","    csrc/fused_dense.cpp:30:63: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, out_features}, input.type());\n","                                                                   ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:33:55: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto lt_workspace = at::empty({1 << 22}, input.type());\n","                                                           ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:35:50: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","                                                      ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:221:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                               \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:223:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n","                                                            ^\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:121:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","                           ^~~~~~~~~~~\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:37:15: warning: unused variable â€˜b_ptrâ€™ [-Wunused-variable]\n","         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n","                   ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp:38:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_bias_forward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:37:15: warning: unused variable â€˜b_ptrâ€™ [-Wunused-variable]\n","         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n","                   ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp:38:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_bias_forward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:37:15: warning: unused variable â€˜b_ptrâ€™ [-Wunused-variable]\n","         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n","                   ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp:38:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_bias_forward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:35:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In function â€˜std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)â€™:\n","    csrc/fused_dense.cpp:64:69: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_weight = at::empty({out_features, in_features}, input.type());\n","                                                                         ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:68:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_bias = at::empty({out_features}, input.type());\n","                                                          ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:70:66: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_input = at::empty({batch_size, in_features}, input.type());\n","                                                                      ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:73:55: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto lt_workspace = at::empty({1 << 22}, input.type());\n","                                                           ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:75:50: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","                                                      ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:221:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                               \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:223:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n","                                                            ^\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:121:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","                           ^~~~~~~~~~~\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:77:15: warning: unused variable â€˜d_b_ptrâ€™ [-Wunused-variable]\n","         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n","                   ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp:78:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_bias_backward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:77:15: warning: unused variable â€˜d_b_ptrâ€™ [-Wunused-variable]\n","         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n","                   ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp:78:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_bias_backward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:77:15: warning: unused variable â€˜d_b_ptrâ€™ [-Wunused-variable]\n","         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n","                   ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp:78:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_bias_backward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:75:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In function â€˜std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)â€™:\n","    csrc/fused_dense.cpp:106:70: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto output1 = at::empty({batch_size, hidden_features}, input.type());\n","                                                                          ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:107:70: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n","                                                                          ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:108:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto output2 = at::empty({batch_size, out_features}, input.type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:111:55: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto lt_workspace = at::empty({1 << 22}, input.type());\n","                                                           ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:113:50: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n","                                                      ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:221:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                               \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:223:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n","                                                            ^\n","    csrc/fused_dense.cpp:113:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:121:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","                           ^~~~~~~~~~~\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:118:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:113:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:118:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:113:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:118:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:113:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In function â€˜std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)â€™:\n","    csrc/fused_dense.cpp:149:73: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n","                                                                             ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:150:74: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n","                                                                              ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:151:58: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_bias1 = at::empty({hidden_features}, input.type());\n","                                                              ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:152:55: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_bias2 = at::empty({out_features}, input.type());\n","                                                           ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:153:66: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_input = at::empty({batch_size, in_features}, input.type());\n","                                                                      ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:154:72: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n","                                                                            ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/fused_dense.cpp:157:55: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto lt_workspace = at::empty({1 << 22}, input.type());\n","                                                           ^\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:159:50: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","                                                      ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:221:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                               \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Tensor.h:3:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:9,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/core/TensorBody.h:338:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ATen.h:13:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/fused_dense.cpp:1:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:223:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n","                                                            ^\n","    csrc/fused_dense.cpp:159:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:121:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","                           ^~~~~~~~~~~\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:162:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:226:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:159:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:162:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:227:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:159:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    csrc/fused_dense.cpp: In lambda function:\n","    csrc/fused_dense.cpp:162:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:66:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPE_USING_HINTâ€™\n","         return __VA_ARGS__();                                                        \\\n","                ^~~~~~~~~~~\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Dispatch.h:228:7: note: in expansion of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n","           ^~~~~~~~~~~~~~~~~~~~\n","    csrc/fused_dense.cpp:159:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n","       ^\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    csrc/fused_dense_cuda.cu(1148): warning: variable \"beta_zero\" was declared but never referenced\n","\n","    csrc/fused_dense_cuda.cu(1272): warning: variable \"alpha\" was declared but never referenced\n","\n","    csrc/fused_dense_cuda.cu(1273): warning: variable \"beta_zero\" was declared but never referenced\n","\n","    csrc/fused_dense_cuda.cu(1274): warning: variable \"status\" was declared but never referenced\n","\n","    csrc/fused_dense_cuda.cu(1329): warning: variable \"alpha\" was declared but never referenced\n","\n","    csrc/fused_dense_cuda.cu(1330): warning: variable \"beta_zero\" was declared but never referenced\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/fused_dense.o build/temp.linux-x86_64-3.7/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/fused_dense_cuda.cpython-37m-x86_64-linux-gnu.so\n","    building 'scaled_upper_triang_masked_softmax_cuda' extension\n","    creating build/temp.linux-x86_64-3.7/csrc/megatron\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-yowt5fg5/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/megatron/scaled_upper_triang_masked_softmax.cpp:18:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-yowt5fg5/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/scaled_upper_triang_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so\n","    building 'scaled_masked_softmax_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-yowt5fg5/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.7/dist-packages/torch/include/ATen/Parallel.h:140:0,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13,\n","                     from /usr/local/lib/python3.7/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/megatron/scaled_masked_softmax.cpp:18:\n","    /usr/local/lib/python3.7/dist-packages/torch/include/ATen/ParallelOpenMP.h:87:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n","     #pragma omp parallel for if ((end - begin) >= grain_size)\n","\n","    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-yowt5fg5/csrc -I/usr/local/lib/python3.7/dist-packages/torch/include -I/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.7/dist-packages/torch/include/TH -I/usr/local/lib/python3.7/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.7m -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.7/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.7/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.7/scaled_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so\n","    running install_lib\n","    copying build/lib.linux-x86_64-3.7/scaled_upper_triang_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    copying build/lib.linux-x86_64-3.7/scaled_masked_softmax_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    copying build/lib.linux-x86_64-3.7/fused_dense_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    creating /usr/local/lib/python3.7/dist-packages/apex\n","    copying build/lib.linux-x86_64-3.7/apex/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex\n","    creating /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply\n","    creating /usr/local/lib/python3.7/dist-packages/apex/transformer\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/enums.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n","    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/arguments.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/standalone_gpt.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/commons.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/testing/global_vars.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/testing\n","    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/p2p_communication.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/_timers.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n","    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/common.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/pipeline_parallel/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel\n","    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/mappings.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/cross_entropy.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/layers.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/random.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/memory.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/tensor_parallel/data.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/parallel_state.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n","    creating /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/functional/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/functional/fused_softmax.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer/functional\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n","    copying build/lib.linux-x86_64-3.7/apex/transformer/microbatches.py -> /usr/local/lib/python3.7/dist-packages/apex/transformer\n","    creating /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/LARC.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/multiproc.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/distributed.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/parallel\n","    creating /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/fp16_utils\n","    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof\n","    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/output.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/base.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/data.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof\n","    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/db.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse\n","    creating /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx\n","    creating /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/amp.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/frontend.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.7/apex/amp/lists/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.7/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.7/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.7/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.7/dist-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.7/apex/amp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/_initialize.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/__version__.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/_amp_state.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/compat.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/rnn_compat.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/scaler.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/utils.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/handle.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/wrap.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.7/apex/amp/opt.py -> /usr/local/lib/python3.7/dist-packages/apex/amp\n","    creating /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n","    copying build/lib.linux-x86_64-3.7/apex/fused_dense/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n","    copying build/lib.linux-x86_64-3.7/apex/fused_dense/fused_dense.py -> /usr/local/lib/python3.7/dist-packages/apex/fused_dense\n","    creating /usr/local/lib/python3.7/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.7/apex/normalization/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.7/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/normalization\n","    creating /usr/local/lib/python3.7/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.7/apex/mlp/mlp.py -> /usr/local/lib/python3.7/dist-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.7/apex/mlp/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/mlp\n","    creating /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/optimizers/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.7/dist-packages/apex/optimizers\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/transducer.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/fmha.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck_module_test.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/test.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/layer_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm\n","    creating /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy\n","    creating /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.7/apex/reparameterization/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.7/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.7/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.7/dist-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.7/apex/_autocast_utils.py -> /usr/local/lib/python3.7/dist-packages/apex\n","    creating /usr/local/lib/python3.7/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.7/apex/RNN/cells.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.7/apex/RNN/__init__.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.7/apex/RNN/models.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.7/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.7/dist-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    copying build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    copying build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    copying build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    copying build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so -> /usr/local/lib/python3.7/dist-packages\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/enums.py to enums.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/arguments.py to arguments.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/commons.py to commons.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/random.py to random.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/tensor_parallel/data.py to data.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/parallel_state.py to parallel_state.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/functional/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/utils.py to utils.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/transformer/microbatches.py to microbatches.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/LARC.py to LARC.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/distributed.py to distributed.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/output.py to output.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/base.py to base.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/prof/data.py to data.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/parse/db.py to db.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/amp.py to amp.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/frontend.py to frontend.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py to _initialize.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/__version__.py to __version__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/compat.py to compat.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/scaler.py to scaler.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/utils.py to utils.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/handle.py to handle.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/wrap.py to wrap.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/amp/opt.py to opt.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fused_dense/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/normalization/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/mlp/mlp.py to mlp.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/mlp/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer/transducer.py to transducer.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/transducer/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha/fmha.py to fmha.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/fmha/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/bottleneck/test.py to test.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/_autocast_utils.py to _autocast_utils.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/cells.py to cells.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/__init__.py to __init__.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/models.py to models.cpython-37.pyc\n","    byte-compiling /usr/local/lib/python3.7/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-37.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing requirements to apex.egg-info/requires.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    adding license file 'LICENSE'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /usr/local/lib/python3.7/dist-packages/apex-0.1-py3.7.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-5i3pgpnb/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/lib/python3.7/dist-packages\n","sysconfig: /usr/lib/python3.7/site-packages\n","Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/lib/python3.7/dist-packages\n","sysconfig: /usr/lib/python3.7/site-packages\n","Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/include/python3.7/UNKNOWN\n","sysconfig: /usr/include/python3.7m/UNKNOWN\n","Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local/bin\n","sysconfig: /usr/bin\n","Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n","distutils: /usr/local\n","sysconfig: /usr\n","Additional context:\n","user = False\n","home = None\n","root = None\n","prefix = None\n","Successfully installed apex-0.1\n","Removed build tracker: '/tmp/pip-req-tracker-rfq01vam'\n","/content/drive/MyDrive/EEG project\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlSOMViMsNrQ","executionInfo":{"elapsed":6097,"status":"ok","timestamp":1636463226094,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"},"user_tz":-480},"outputId":"4933e7b5-eed2-44ee-f780-7d8267ba13fd"},"source":["!pip install mne"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mne\n","  Downloading mne-0.24.0-py3-none-any.whl (7.4 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.4 MB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n","Installing collected packages: mne\n","Successfully installed mne-0.24.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0BWaTwarpnJ","executionInfo":{"elapsed":1079,"status":"ok","timestamp":1636463229009,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"},"user_tz":-480},"outputId":"863c34ca-2b82-4441-a208-367608743d4c"},"source":["import torch\n","flag = torch.cuda.is_available()\n","if flag:\n","    print(\"CUDAå¯ä½¿ç”¨\")\n","else:\n","    print(\"CUDAä¸å¯ç”¨\")\n","print(torch.cuda.device_count())"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["CUDAå¯ä½¿ç”¨\n","1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7OoPnZ1E5wAI","executionInfo":{"status":"ok","timestamp":1636470974184,"user_tz":-480,"elapsed":406,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}},"outputId":"07389f72-1834-4991-e1f3-b04c3a3cf40d"},"source":["%cd '/content/drive/MyDrive/EEG project'"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/EEG project\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNarSjEO0S_B","executionInfo":{"status":"ok","timestamp":1636470989480,"user_tz":-480,"elapsed":1127,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}},"outputId":"451ddf8a-3d1d-4a12-8dfb-b6df5cd3a8fd"},"source":["from PIL import Image\n","import numpy as np\n"," \n","I = Image.open('æ¯•ä¸šè¯.jpg')   \n","I_array = np.array(I)\n","print(type(I))\n","print(I_array)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'PIL.JpegImagePlugin.JpegImageFile'>\n","[[[77 29 17]\n","  [76 28 16]\n","  [75 27 15]\n","  ...\n","  [34 25 28]\n","  [33 27 29]\n","  [34 28 28]]\n","\n"," [[79 31 19]\n","  [78 30 18]\n","  [76 28 16]\n","  ...\n","  [33 24 27]\n","  [31 25 25]\n","  [31 25 25]]\n","\n"," [[80 32 20]\n","  [80 32 20]\n","  [79 31 19]\n","  ...\n","  [36 27 28]\n","  [34 28 28]\n","  [33 27 27]]\n","\n"," ...\n","\n"," [[23 14  7]\n","  [22 13  6]\n","  [22 13  6]\n","  ...\n","  [61 33 21]\n","  [61 33 21]\n","  [61 33 21]]\n","\n"," [[24 13  7]\n","  [23 12  6]\n","  [23 12  6]\n","  ...\n","  [62 34 22]\n","  [62 34 22]\n","  [62 34 22]]\n","\n"," [[23 12  6]\n","  [23 12  6]\n","  [23 12  6]\n","  ...\n","  [63 35 24]\n","  [64 36 25]\n","  [64 36 25]]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rf8MNjOM02JD","executionInfo":{"status":"ok","timestamp":1636470997004,"user_tz":-480,"elapsed":3888,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}},"outputId":"7c56e4c5-871d-491e-df5c-9136e246b95a"},"source":["train_X = np.load(file=\"train_eeg.npy\")\n","a = train_X[0]\n","a"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 1.19e-04,  7.70e-05,  2.20e-05],\n","        [ 1.19e-04,  1.02e-04,  5.80e-05],\n","        [ 1.16e-04,  1.15e-04,  7.90e-05],\n","        ...,\n","        [ 4.70e-05,  5.80e-05,  3.10e-05],\n","        [ 8.40e-05,  8.00e-05,  6.00e-05],\n","        [ 9.40e-05,  9.50e-05,  6.90e-05]],\n","\n","       [[ 8.30e-05,  8.30e-05,  3.20e-05],\n","        [ 1.30e-04,  1.19e-04,  6.00e-05],\n","        [ 1.34e-04,  1.25e-04,  8.70e-05],\n","        ...,\n","        [ 3.80e-05,  5.40e-05, -3.00e-06],\n","        [ 4.70e-05,  6.30e-05,  2.10e-05],\n","        [ 5.30e-05,  5.40e-05,  1.30e-05]],\n","\n","       [[ 6.80e-05,  5.50e-05,  3.20e-05],\n","        [ 5.40e-05,  5.80e-05,  2.50e-05],\n","        [ 9.20e-05,  1.03e-04,  6.20e-05],\n","        ...,\n","        [ 1.19e-04,  1.24e-04,  8.70e-05],\n","        [ 1.29e-04,  1.36e-04,  9.00e-05],\n","        [ 1.09e-04,  1.04e-04,  6.30e-05]],\n","\n","       ...,\n","\n","       [[ 8.00e-06, -3.00e-06, -1.60e-05],\n","        [-5.00e-06,  7.00e-06,  1.00e-06],\n","        [-3.70e-05, -1.60e-05, -2.40e-05],\n","        ...,\n","        [-3.60e-05, -2.40e-05, -4.50e-05],\n","        [-7.10e-05, -5.30e-05, -6.40e-05],\n","        [-5.20e-05, -3.20e-05, -6.00e-05]],\n","\n","       [[-6.00e-06,  8.00e-06, -3.10e-05],\n","        [-1.80e-05,  6.00e-06, -2.90e-05],\n","        [-3.70e-05, -2.30e-05, -5.00e-05],\n","        ...,\n","        [-4.50e-05, -2.70e-05, -6.20e-05],\n","        [-5.80e-05, -4.00e-05, -8.70e-05],\n","        [-6.20e-05, -5.10e-05, -1.03e-04]],\n","\n","       [[-2.60e-05, -4.60e-05, -8.70e-05],\n","        [-7.70e-05, -9.80e-05, -1.37e-04],\n","        [-8.60e-05, -7.70e-05, -1.11e-04],\n","        ...,\n","        [-3.50e-05, -5.80e-05, -7.00e-05],\n","        [-5.00e-05, -6.10e-05, -6.50e-05],\n","        [-5.20e-05, -6.10e-05, -6.70e-05]]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"5C5rUxKK2mOW","executionInfo":{"status":"ok","timestamp":1636471128980,"user_tz":-480,"elapsed":13391,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}}},"source":["import torchvision\n","randomresizedcrop = torchvision.transforms.RandomResizedCrop(\n","    224,\n","    scale=(0.05, 0.14),\n",")\n","# trans.extend([transforms.Compose([\n","#   #   transforms.ToPILImage(),#è½¬å˜æˆå›¾åƒç±»åž‹\n","#     randomresizedcrop,\n","#     transforms.RandomHorizontalFlip(p=0.5),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize(mean=mean, std=std)])\n","# a = torchvision.transforms.ToPILImage()(a)\n","# img = randomresizedcrop(a)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrHcMDLsGPP5","executionInfo":{"status":"ok","timestamp":1636471018269,"user_tz":-480,"elapsed":511,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}}},"source":["def normalization(data):\n","  _range = np.max(data) - np.min(data)\n","  return (data - np.min(data)) / _range\n","def preprocess(data):\n","  ampl_data = data*100000\n","  norm_data = normalization(ampl_data)\n","  rgb_data = norm_data*255\n","  rgb_data = rgb_data.astype(np.uint8)\n","  return rgb_data\n","\n","rgb_data = preprocess(train_X)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmUq-KPvSpGR","executionInfo":{"status":"ok","timestamp":1636471021045,"user_tz":-480,"elapsed":439,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}},"outputId":"438eabdd-6223-4f25-80b4-b08c12e2d4e7"},"source":["rgb_data.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6960, 32, 20, 3)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"4eDk8dO7Q6HS","executionInfo":{"status":"ok","timestamp":1636471165787,"user_tz":-480,"elapsed":425,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}}},"source":["b = torchvision.transforms.ToPILImage()(rgb_data[0])\n","c = randomresizedcrop(b)\n","d = torchvision.transforms.RandomHorizontalFlip(p=0.5)(c)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fY2jyP4aSGFM","executionInfo":{"status":"ok","timestamp":1636471258827,"user_tz":-480,"elapsed":703,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"}},"outputId":"7899c1e1-f3d8-442f-d5ce-65cf8c7920f4"},"source":["d.size"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(224, 224)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UUbKb1i2snWl","outputId":"a9a2a503-1eb3-4fb7-d94e-e51e8eb0538a"},"source":["!python -m torch.distributed.launch --nproc_per_node=1 swav_impl.py --epochs 10 --base_lr 0.6 --final_lr 0.0006 --warmup_epochs 0 --batch_size 32 --size_crops 224 96 --nmb_crops 2 6 --min_scale_crops 0.14 0.05 --max_scale_crops 1. 0.14 --use_fp16 true --freeze_prototypes_niters 5005 --queue_length 3840 --epoch_queue_starts 15"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:164: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n","  \"The module torch.distributed.launch is deprecated \"\n","The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run\n","WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.\n"," Please read local_rank from `os.environ('LOCAL_RANK')` instead.\n","INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:\n","  entrypoint       : swav_impl.py\n","  min_nodes        : 1\n","  max_nodes        : 1\n","  nproc_per_node   : 1\n","  run_id           : none\n","  rdzv_backend     : static\n","  rdzv_endpoint    : 127.0.0.1:29500\n","  rdzv_configs     : {'rank': 0, 'timeout': 900}\n","  max_restarts     : 3\n","  monitor_interval : 5\n","  log_dir          : None\n","  metrics_cfg      : {}\n","\n","INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_mbokuk0a/none_0mv8h1hp\n","INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3\n","INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group\n","/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:53: FutureWarning: This is an experimental API and will be changed in future.\n","  \"This is an experimental API and will be changed in future.\", FutureWarning\n","INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:\n","  restart_count=0\n","  master_addr=127.0.0.1\n","  master_port=29500\n","  group_rank=0\n","  group_world_size=1\n","  local_ranks=[0]\n","  role_ranks=[0]\n","  global_ranks=[0]\n","  role_world_sizes=[1]\n","  global_world_sizes=[1]\n","\n","INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group\n","INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mbokuk0a/none_0mv8h1hp/attempt_0/0/error.json\n","INFO - 11/09/21 13:07:31 - 0:00:00 - ============ Initialized logger ============\n","INFO - 11/09/21 13:07:31 - 0:00:00 - arch: resnet18\n","                                     base_lr: 0.6\n","                                     batch_size: 32\n","                                     checkpoint_freq: 25\n","                                     crops_for_assign: [0, 1]\n","                                     dist_url: env://\n","                                     dump_checkpoints: ./checkpoints\n","                                     dump_path: .\n","                                     epoch_queue_starts: 15\n","                                     epochs: 10\n","                                     epsilon: 0.05\n","                                     feat_dim: 128\n","                                     final_lr: 0.0006\n","                                     freeze_prototypes_niters: 5005\n","                                     gpu_to_work_on: 0\n","                                     hidden_mlp: 2048\n","                                     is_slurm_job: False\n","                                     local_rank: 0\n","                                     max_scale_crops: [1.0, 0.14]\n","                                     min_scale_crops: [0.14, 0.05]\n","                                     nmb_crops: [2, 6]\n","                                     nmb_prototypes: 3000\n","                                     queue_length: 3840\n","                                     rank: 0\n","                                     seed: 31\n","                                     sinkhorn_iterations: 3\n","                                     size_crops: [224, 96]\n","                                     start_warmup: 0\n","                                     sync_bn: pytorch\n","                                     syncbn_process_group_size: 8\n","                                     temperature: 0.1\n","                                     use_fp16: True\n","                                     warmup_epochs: 0\n","                                     wd: 1e-06\n","                                     workers: 10\n","                                     world_size: 1\n","INFO - 11/09/21 13:07:31 - 0:00:00 - The experiment will be stored in .\n","                                     \n","\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","INFO - 11/09/21 13:07:31 - 0:00:00 - Building data done with 6960 images loaded.\n","INFO - 11/09/21 13:07:41 - 0:00:10 - ResNet(\n","                                       (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)\n","                                       (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)\n","                                       (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                       (relu): ReLU(inplace=True)\n","                                       (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","                                       (layer1): Sequential(\n","                                         (0): BasicBlock(\n","                                           (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                         )\n","                                         (1): BasicBlock(\n","                                           (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                         )\n","                                       )\n","                                       (layer2): Sequential(\n","                                         (0): BasicBlock(\n","                                           (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (downsample): Sequential(\n","                                             (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                                             (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           )\n","                                         )\n","                                         (1): BasicBlock(\n","                                           (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                         )\n","                                       )\n","                                       (layer3): Sequential(\n","                                         (0): BasicBlock(\n","                                           (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (downsample): Sequential(\n","                                             (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                                             (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           )\n","                                         )\n","                                         (1): BasicBlock(\n","                                           (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                         )\n","                                       )\n","                                       (layer4): Sequential(\n","                                         (0): BasicBlock(\n","                                           (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (downsample): Sequential(\n","                                             (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","                                             (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           )\n","                                         )\n","                                         (1): BasicBlock(\n","                                           (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                           (relu): ReLU(inplace=True)\n","                                           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","                                           (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                         )\n","                                       )\n","                                       (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","                                       (projection_head): Sequential(\n","                                         (0): Linear(in_features=512, out_features=2048, bias=True)\n","                                         (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","                                         (2): ReLU(inplace=True)\n","                                         (3): Linear(in_features=2048, out_features=128, bias=True)\n","                                       )\n","                                       (prototypes): Linear(in_features=128, out_features=3000, bias=False)\n","                                     )\n","INFO - 11/09/21 13:07:41 - 0:00:10 - Building model done.\n","INFO - 11/09/21 13:07:41 - 0:00:10 - Building optimizer done.\n","Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","INFO - 11/09/21 13:07:41 - 0:00:10 - Initializing mixed precision done.\n","INFO - 11/09/21 13:07:41 - 0:00:10 - ============ Starting epoch 0 ... ============\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","INFO - 11/09/21 13:07:49 - 0:00:18 - Epoch: [0][0]\tTime 7.692 (7.692)\tData 5.369 (5.369)\tLoss 8.1138 (8.1138)\tLr: 0.6000\n","INFO - 11/09/21 13:07:49 - 0:00:18 - Reducer buckets have been rebuilt in this iteration.\n","INFO - 11/09/21 13:08:06 - 0:00:35 - Epoch: [0][50]\tTime 0.345 (0.488)\tData 0.007 (0.110)\tLoss 7.5990 (7.7070)\tLr: 0.5992\n","INFO - 11/09/21 13:08:24 - 0:00:53 - Epoch: [0][100]\tTime 0.401 (0.420)\tData 0.013 (0.059)\tLoss 7.5924 (7.6510)\tLr: 0.5969\n","INFO - 11/09/21 13:08:41 - 0:01:10 - Epoch: [0][150]\tTime 0.361 (0.396)\tData 0.007 (0.041)\tLoss 7.6908 (7.6308)\tLr: 0.5930\n","INFO - 11/09/21 13:08:58 - 0:01:27 - Epoch: [0][200]\tTime 0.197 (0.382)\tData 0.000 (0.033)\tLoss 7.5623 (7.6248)\tLr: 0.5875\n","INFO - 11/09/21 13:09:02 - 0:01:31 - ============ Starting epoch 1 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:09:07 - 0:01:36 - Epoch: [1][0]\tTime 4.424 (4.424)\tData 3.403 (3.403)\tLoss 7.5313 (7.5313)\tLr: 0.5853\n","INFO - 11/09/21 13:09:27 - 0:01:56 - Epoch: [1][50]\tTime 0.397 (0.482)\tData 0.005 (0.073)\tLoss 7.5699 (7.5837)\tLr: 0.5779\n","INFO - 11/09/21 13:09:45 - 0:02:14 - Epoch: [1][100]\tTime 0.380 (0.419)\tData 0.022 (0.040)\tLoss 7.5752 (7.5700)\tLr: 0.5690\n","INFO - 11/09/21 13:10:02 - 0:02:31 - Epoch: [1][150]\tTime 0.319 (0.396)\tData 0.000 (0.029)\tLoss 7.5440 (7.5660)\tLr: 0.5587\n","INFO - 11/09/21 13:10:19 - 0:02:48 - Epoch: [1][200]\tTime 0.210 (0.382)\tData 0.000 (0.023)\tLoss 7.5192 (7.5587)\tLr: 0.5470\n","INFO - 11/09/21 13:10:23 - 0:02:52 - ============ Starting epoch 2 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:10:29 - 0:02:58 - Epoch: [2][0]\tTime 6.096 (6.096)\tData 5.356 (5.356)\tLoss 7.4835 (7.4835)\tLr: 0.5428\n","INFO - 11/09/21 13:10:48 - 0:03:17 - Epoch: [2][50]\tTime 0.280 (0.481)\tData 0.005 (0.111)\tLoss 7.5572 (7.5091)\tLr: 0.5294\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","INFO - 11/09/21 13:11:05 - 0:03:35 - Epoch: [2][100]\tTime 0.314 (0.420)\tData 0.018 (0.060)\tLoss 7.5083 (7.5252)\tLr: 0.5148\n","INFO - 11/09/21 13:11:23 - 0:03:52 - Epoch: [2][150]\tTime 0.360 (0.399)\tData 0.016 (0.042)\tLoss 7.5541 (7.5223)\tLr: 0.4991\n","INFO - 11/09/21 13:11:40 - 0:04:09 - Epoch: [2][200]\tTime 0.191 (0.384)\tData 0.000 (0.033)\tLoss 7.5859 (7.5263)\tLr: 0.4824\n","INFO - 11/09/21 13:11:44 - 0:04:13 - ============ Starting epoch 3 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:11:48 - 0:04:17 - Epoch: [3][0]\tTime 4.258 (4.258)\tData 3.059 (3.059)\tLoss 7.5083 (7.5083)\tLr: 0.4765\n","INFO - 11/09/21 13:12:09 - 0:04:38 - Epoch: [3][50]\tTime 0.380 (0.482)\tData 0.012 (0.066)\tLoss 7.5534 (7.5028)\tLr: 0.4585\n","INFO - 11/09/21 13:12:26 - 0:04:55 - Epoch: [3][100]\tTime 0.424 (0.419)\tData 0.007 (0.037)\tLoss 7.4602 (7.4917)\tLr: 0.4396\n","INFO - 11/09/21 13:12:44 - 0:05:13 - Epoch: [3][150]\tTime 0.377 (0.397)\tData 0.006 (0.027)\tLoss 7.5374 (7.4996)\tLr: 0.4201\n","INFO - 11/09/21 13:13:01 - 0:05:30 - Epoch: [3][200]\tTime 0.190 (0.384)\tData 0.000 (0.022)\tLoss 7.5745 (7.4989)\tLr: 0.3999\n","INFO - 11/09/21 13:13:05 - 0:05:34 - ============ Starting epoch 4 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:13:10 - 0:05:39 - Epoch: [4][0]\tTime 4.585 (4.585)\tData 3.431 (3.431)\tLoss 7.4763 (7.4763)\tLr: 0.3929\n","INFO - 11/09/21 13:13:30 - 0:05:59 - Epoch: [4][50]\tTime 0.344 (0.493)\tData 0.014 (0.074)\tLoss 7.5097 (7.4889)\tLr: 0.3721\n","INFO - 11/09/21 13:13:48 - 0:06:17 - Epoch: [4][100]\tTime 0.358 (0.426)\tData 0.000 (0.041)\tLoss 7.5006 (7.4933)\tLr: 0.3508\n","INFO - 11/09/21 13:14:06 - 0:06:35 - Epoch: [4][150]\tTime 0.317 (0.403)\tData 0.000 (0.030)\tLoss 7.4760 (7.4873)\tLr: 0.3293\n","INFO - 11/09/21 13:14:23 - 0:06:53 - Epoch: [4][200]\tTime 0.188 (0.390)\tData 0.000 (0.024)\tLoss 7.4789 (7.4816)\tLr: 0.3077\n","INFO - 11/09/21 13:14:27 - 0:06:56 - ============ Starting epoch 5 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:14:32 - 0:07:01 - Epoch: [5][0]\tTime 4.521 (4.521)\tData 3.370 (3.370)\tLoss 7.4570 (7.4570)\tLr: 0.3003\n","INFO - 11/09/21 13:14:52 - 0:07:21 - Epoch: [5][50]\tTime 0.365 (0.490)\tData 0.005 (0.075)\tLoss 7.4229 (7.4716)\tLr: 0.2786\n","INFO - 11/09/21 13:15:10 - 0:07:39 - Epoch: [5][100]\tTime 0.378 (0.425)\tData 0.006 (0.041)\tLoss 7.4731 (7.4607)\tLr: 0.2571\n","INFO - 11/09/21 13:15:28 - 0:07:57 - Epoch: [5][150]\tTime 0.314 (0.402)\tData 0.000 (0.030)\tLoss 7.4430 (7.4639)\tLr: 0.2357\n","INFO - 11/09/21 13:15:45 - 0:08:14 - Epoch: [5][200]\tTime 0.190 (0.389)\tData 0.000 (0.024)\tLoss 7.4219 (7.4587)\tLr: 0.2147\n","INFO - 11/09/21 13:15:49 - 0:08:18 - ============ Starting epoch 6 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:15:54 - 0:08:23 - Epoch: [6][0]\tTime 4.558 (4.558)\tData 3.452 (3.452)\tLoss 7.4136 (7.4136)\tLr: 0.2077\n","INFO - 11/09/21 13:16:14 - 0:08:43 - Epoch: [6][50]\tTime 0.371 (0.487)\tData 0.008 (0.075)\tLoss 7.4099 (7.4132)\tLr: 0.1873\n","INFO - 11/09/21 13:16:32 - 0:09:01 - Epoch: [6][100]\tTime 0.357 (0.422)\tData 0.000 (0.042)\tLoss 7.4666 (7.4172)\tLr: 0.1675\n","INFO - 11/09/21 13:16:49 - 0:09:19 - Epoch: [6][150]\tTime 0.342 (0.399)\tData 0.001 (0.030)\tLoss 7.3828 (7.4132)\tLr: 0.1485\n","INFO - 11/09/21 13:17:07 - 0:09:36 - Epoch: [6][200]\tTime 0.189 (0.385)\tData 0.000 (0.024)\tLoss 7.4044 (7.4147)\tLr: 0.1302\n","INFO - 11/09/21 13:17:10 - 0:09:39 - ============ Starting epoch 7 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:17:15 - 0:09:44 - Epoch: [7][0]\tTime 4.543 (4.543)\tData 3.406 (3.406)\tLoss 7.4040 (7.4040)\tLr: 0.1241\n","INFO - 11/09/21 13:17:35 - 0:10:04 - Epoch: [7][50]\tTime 0.403 (0.487)\tData 0.005 (0.075)\tLoss 7.3764 (7.4308)\tLr: 0.1071\n","INFO - 11/09/21 13:17:53 - 0:10:22 - Epoch: [7][100]\tTime 0.343 (0.422)\tData 0.007 (0.041)\tLoss 7.4888 (7.4266)\tLr: 0.0910\n","INFO - 11/09/21 13:18:11 - 0:10:40 - Epoch: [7][150]\tTime 0.295 (0.399)\tData 0.008 (0.030)\tLoss 7.4152 (7.4227)\tLr: 0.0760\n","INFO - 11/09/21 13:18:28 - 0:10:57 - Epoch: [7][200]\tTime 0.190 (0.386)\tData 0.000 (0.024)\tLoss 7.3564 (7.4188)\tLr: 0.0622\n","INFO - 11/09/21 13:18:32 - 0:11:01 - ============ Starting epoch 8 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:18:36 - 0:11:05 - Epoch: [8][0]\tTime 4.513 (4.513)\tData 3.368 (3.368)\tLoss 7.3921 (7.3921)\tLr: 0.0578\n","INFO - 11/09/21 13:18:56 - 0:11:25 - Epoch: [8][50]\tTime 0.366 (0.483)\tData 0.013 (0.072)\tLoss 7.4295 (7.4088)\tLr: 0.0457\n","INFO - 11/09/21 13:19:14 - 0:11:43 - Epoch: [8][100]\tTime 0.299 (0.421)\tData 0.006 (0.039)\tLoss 7.3887 (7.4018)\tLr: 0.0350\n","INFO - 11/09/21 13:19:32 - 0:12:01 - Epoch: [8][150]\tTime 0.383 (0.399)\tData 0.000 (0.028)\tLoss 7.4225 (7.4033)\tLr: 0.0256\n","INFO - 11/09/21 13:19:49 - 0:12:18 - Epoch: [8][200]\tTime 0.192 (0.386)\tData 0.000 (0.023)\tLoss 7.4157 (7.4048)\tLr: 0.0176\n","INFO - 11/09/21 13:19:53 - 0:12:22 - ============ Starting epoch 9 ... ============\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","INFO - 11/09/21 13:19:58 - 0:12:27 - Epoch: [9][0]\tTime 4.644 (4.644)\tData 3.499 (3.499)\tLoss 7.4102 (7.4102)\tLr: 0.0153\n","INFO - 11/09/21 13:20:18 - 0:12:47 - Epoch: [9][50]\tTime 0.354 (0.484)\tData 0.006 (0.075)\tLoss 7.3706 (7.4114)\tLr: 0.0093\n","INFO - 11/09/21 13:20:35 - 0:13:05 - Epoch: [9][100]\tTime 0.358 (0.421)\tData 0.000 (0.041)\tLoss 7.3168 (7.4069)\tLr: 0.0049\n","INFO - 11/09/21 13:20:53 - 0:13:22 - Epoch: [9][150]\tTime 0.362 (0.399)\tData 0.007 (0.030)\tLoss 7.3476 (7.4081)\tLr: 0.0020\n","INFO - 11/09/21 13:21:10 - 0:13:39 - Epoch: [9][200]\tTime 0.189 (0.385)\tData 0.000 (0.024)\tLoss 7.4555 (7.4089)\tLr: 0.0007\n","INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.\n","INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish\n","/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/utils/store.py:71: FutureWarning: This is an experimental API and will be changed in future.\n","  \"This is an experimental API and will be changed in future.\", FutureWarning\n","INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0005230903625488281 seconds\n","{\"name\": \"torchelastic.worker.status.SUCCEEDED\", \"source\": \"WORKER\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": 0, \"group_rank\": 0, \"worker_id\": \"920\", \"role\": \"default\", \"hostname\": \"8f4b9e2cfa0a\", \"state\": \"SUCCEEDED\", \"total_run_time\": 830, \"rdzv_backend\": \"static\", \"raw_error\": null, \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\", \\\"local_rank\\\": [0], \\\"role_rank\\\": [0], \\\"role_world_size\\\": [1]}\", \"agent_restarts\": 0}}\n","{\"name\": \"torchelastic.worker.status.SUCCEEDED\", \"source\": \"AGENT\", \"timestamp\": 0, \"metadata\": {\"run_id\": \"none\", \"global_rank\": null, \"group_rank\": 0, \"worker_id\": null, \"role\": \"default\", \"hostname\": \"8f4b9e2cfa0a\", \"state\": \"SUCCEEDED\", \"total_run_time\": 830, \"rdzv_backend\": \"static\", \"raw_error\": null, \"metadata\": \"{\\\"group_world_size\\\": 1, \\\"entry_point\\\": \\\"python3\\\"}\", \"agent_restarts\": 0}}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5QDGH_WZngP","executionInfo":{"elapsed":419,"status":"ok","timestamp":1636365975851,"user":{"displayName":"junhao li","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16774127106048556442"},"user_tz":-480},"outputId":"60c5fddf-647f-4c5a-a688-9d96c428196a"},"source":["print(torch.__version__)\n","print(torch.version.cuda)\n","print(torch.backends.cudnn.version())\n","print(torch.cuda.get_device_name(0))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1.9.0+cu111\n","11.1\n","8005\n","Tesla P100-PCIE-16GB\n"]}]}]}